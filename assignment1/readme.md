Remark:

- Assignment 1:
  - Q1: `knn.ipynb`
    - kNN implmentation
  - Q2 & Q3: `svm.ipynb` and `softmax.ipynb`
    - (Loop + tensor) implementation of (SVM + Softmax) (loss & gradient) derivations.
    - Gradient check (numerical vs analytic) analysis
    - Learning rate + regularization factor experiment and choosing.
    - Training function: generate batches, stochastic gradient descent
  - Q4: `two_layer_net.ipynb`
    - Tensor implementation of (loss & gradient) derivations for a 2-layer neural network
    - generate batches, stochastic gradient descent (W1, W2, b1, b2)
    - tune hyper parameter
  - Q5: `features.ipynb`
    - Apply different models on HOG and HSV features
  - Q6 Bonus:
    - in `two_layer_net.ipynb`, tune hyperparameters or apply other strategies to achieve some goal
    - in `features.ipynb`, (1) design and implement your own features and compare the performance with the baseline (2) Do something interesting using the material and code in this assignment
